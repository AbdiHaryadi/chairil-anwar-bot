{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, Dense, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import string\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sentences(filepath):\n",
    "    def special_chars_to_words(sentence):\n",
    "        special_char_dict = {\n",
    "            \"0\": \"nol\",\n",
    "            \"1\": \"satu\",\n",
    "            \"2\": \"dua\",\n",
    "            \"3\": \"tiga\",\n",
    "            \"4\": \"empat\",\n",
    "            \"5\": \"lima\",\n",
    "            \"6\": \"enam\",\n",
    "            \"7\": \"tujuh\",\n",
    "            \"8\": \"delapan\",\n",
    "            \"9\": \"sembilan\",\n",
    "            \"\\n\": \" newline \"\n",
    "        }\n",
    "        \n",
    "        new_sentence = \"\"\n",
    "        for c in sentence:\n",
    "            if c in special_char_dict.keys():\n",
    "                new_sentence += special_char_dict[c]\n",
    "            else:\n",
    "                new_sentence += c\n",
    "                \n",
    "        return new_sentence\n",
    "        \n",
    "    VALID_CHARACTERS = string.ascii_letters + \" -\\n\" + string.digits\n",
    "    SPACEABLE_CHARACTER = \"-\"\n",
    "    try:\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read().strip() + \"\\n\\n\"\n",
    "            \n",
    "        # Remove invalid characters\n",
    "        modified_content = \"\".join([c for c in content if c in VALID_CHARACTERS])\n",
    "        \n",
    "        # Change spaceable character to space\n",
    "        modified_content = \"\".join([\" \" if c in SPACEABLE_CHARACTER else c for c in modified_content])\n",
    "        \n",
    "        # Change special characters to words\n",
    "        modified_content = special_chars_to_words(modified_content)\n",
    "        \n",
    "        # Remove double spaces\n",
    "        modified_content = modified_content.replace(\"  \", \" \")\n",
    "        \n",
    "        \"\"\"\n",
    "        # Get nonzero and non-one-word bait as sentence\n",
    "        sentences = modified_content.split(\"\\n\\n\")\n",
    "        sentences = list(map(lambda sentence: (sentence + (\"\" if sentence.endswith(\"\\n\") else \"\\n\")).replace(\"\\n\", \" \"), sentences))\n",
    "        sentences = list(filter(lambda sentence: sentence != \"\", sentences))\n",
    "        sentences = list(filter(lambda sentence: \" \" in sentence.strip(), sentences)) # Jika tidak ada spasi, berarti satu kata\n",
    "        \"\"\"\n",
    "        \n",
    "        # return sentences\n",
    "        return [modified_content]\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(\"Warning: File not found\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded aku.txt\n",
      "Loaded cerita-buat-dien-tamaela.txt\n",
      "Loaded cinta-dan-benci.txt\n",
      "Loaded cintaku-jauh-di-pulau.txt\n",
      "Loaded derai-derai-cemara.txt\n",
      "Loaded di-mesjid.txt\n",
      "Loaded diponegoro.txt\n",
      "Loaded doa.txt\n",
      "Loaded hampa.txt\n",
      "Loaded kawanku-dan-aku.txt\n",
      "Loaded kepada-kawan.txt\n",
      "Loaded kepada-peminta-minta.txt\n",
      "Loaded krawang-bekasi.txt\n",
      "Loaded lagu-siul.txt\n",
      "Loaded nisan.txt\n",
      "Loaded persetujuan-dengan-bung-karno.txt\n",
      "Loaded prajurit-jaga-malam.txt\n",
      "Loaded puisi-kehidupan.txt\n",
      "Loaded rumahku.txt\n",
      "Loaded sajak-putih.txt\n",
      "Loaded sebuah-kamar.txt\n",
      "Loaded selamat-tinggal.txt\n",
      "Loaded senja-di-pelabuhan-kecil.txt\n",
      "Loaded sia-sia.txt\n",
      "Loaded tak-sepadan.txt\n",
      "Loaded tuti-artic.txt\n",
      "Loaded yang-terampas-dan-yang-terputus.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "my_sentences = []\n",
    "PATH = \"data/chairil-anwar/\"\n",
    "for file in os.listdir(PATH):\n",
    "    if file.endswith(\".txt\"):\n",
    "        print(\"Loaded {}\".format(file))\n",
    "        my_sentences += load_sentences(PATH + file)\n",
    "\n",
    "# my_sentences = load_sentences(\"data/chairil-anwar/puisi-kehidupan.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(my_sentences)\n",
    "total_words = len(tokenizer.word_index) + 1 # Kata kosong termasuk (yaitu token 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = []\n",
    "tokens = tokenizer.texts_to_sequences(my_sentences)\n",
    "for token in tokens:\n",
    "    for i in range(1, len(token)):\n",
    "        n_gram_sequence = token[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "220"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sequence_len = max([len(sequence) for sequence in input_sequences])\n",
    "max_sequence_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen = max_sequence_len, padding = \"pre\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = input_sequences[:,:-1]\n",
    "\n",
    "labels = input_sequences[:,-1]\n",
    "ys = tf.keras.utils.to_categorical(labels, num_classes = total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Embedding(total_words, 128, input_length = max_sequence_len - 1),\n",
    "    Bidirectional(LSTM(128)),\n",
    "    Dense(total_words, activation = \"softmax\")\n",
    "])\n",
    "adam = Adam(lr = 0.01) # lr: learning rate\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=adam, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2583/2583 [==============================] - 127s 49ms/sample - loss: 6.3626 - acc: 0.1882\n",
      "Epoch 2/10\n",
      "2583/2583 [==============================] - 126s 49ms/sample - loss: 5.4434 - acc: 0.2168\n",
      "Epoch 3/10\n",
      "2583/2583 [==============================] - 131s 51ms/sample - loss: 4.9135 - acc: 0.2191\n",
      "Epoch 4/10\n",
      "2583/2583 [==============================] - 131s 51ms/sample - loss: 4.3448 - acc: 0.2408\n",
      "Epoch 5/10\n",
      "2583/2583 [==============================] - 136s 53ms/sample - loss: 3.6567 - acc: 0.2946\n",
      "Epoch 6/10\n",
      "2583/2583 [==============================] - 141s 54ms/sample - loss: 2.8892 - acc: 0.3914\n",
      "Epoch 7/10\n",
      "1824/2583 [====================>.........] - ETA: 44s - loss: 2.0650 - acc: 0.5389"
     ]
    }
   ],
   "source": [
    "history = model.fit(xs, ys, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".Bait 1 selesai\n",
      "..............................................................................Bait 2 selesai\n",
      "....Bait 3 selesai\n",
      "...................................Bait 4 selesai\n",
      "....Bait 5 selesai\n",
      "..............................................................Bait 6 selesai\n",
      "...Bait 7 selesai\n",
      "..........................................................................Bait 8 selesai\n",
      ".\n",
      "masihkah aku diberi kesempatan \n",
      " \n",
      " hari ini aku menuju satu puncak tangga yang baru \n",
      " karena aku akan membuka lembaran baru \n",
      " untuk sisa jatah umurku yang baru \n",
      " daun gugur satu satu \n",
      " semua terjadi karena ijin allah \n",
      " tapi coba aku tengok kebelakang \n",
      " ternyata aku masih banyak berhutang \n",
      " ya berhutang pada diriku \n",
      " karena ibadahku masih pas pasan \n",
      " kuraba dahiku \n",
      " astagfirullah sujudku masih jauh dari khusyuk \n",
      " kutimbang keinginanku \n",
      " hmm masih lebih besar duniawiku \n",
      " \n",
      " ya allah \n",
      " \n",
      " akankah aku masih bertemu tanggal dan bulan yang sama di tahun depan \n",
      " akankah aku masih merasakan rasa ini pada tanggal dan bulan yang sama di tahun depan \n",
      " masihkah aku diberi kesempatan \n",
      " \n",
      " ya allah \n",
      " \n",
      " tetes airmataku adalah tanda kelemahanku \n",
      " rasa sedih yang mendalam adalah penyesalanku \n",
      " astagfirullah \n",
      " jika engkau ijinkan hamba bertemu tahun depan \n",
      " ijinkan hambamu ini mulai hari ini lebih khusyuk dalam ibadah \n",
      " timbangan dunia dan akhirat hamba seimbang \n",
      " sehingga hamba bisa sempurna sebagai khalifahmu \n",
      " hamba sangat ingin melihat wajahmu di sana \n",
      " ya allah \n",
      " ijikanlah \n",
      " \n",
      " ijikanlah \n",
      " \n",
      " tetes airmataku adalah tanda kelemahanku \n",
      " rasa sedih yang mendalam adalah penyesalanku \n",
      " astagfirullah \n",
      " jika engkau ijinkan hamba bertemu tahun depan \n",
      " ijinkan hambamu ini mulai hari ini lebih khusyuk dalam ibadah \n",
      " timbangan dunia dan akhirat hamba seimbang \n",
      " sehingga hamba bisa sempurna sebagai khalifahmu \n",
      " hamba sangat ingin melihat wajahmu di sana \n",
      " hamba sangat ingin melihat senyummu di sana \n",
      " ya allah \n",
      " ijikanlah \n",
      " ijikanlah \n",
      " ijikanlah \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "seed_text = \"masihkah aku diberi kesempatan\".lower()\n",
    "bait = 0\n",
    "word_count = 0\n",
    "already_newline = False\n",
    "already_bait_baru = False\n",
    "\n",
    "while bait < 8 and word_count < 1000:\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen = max_sequence_len - 1, padding = \"pre\")\n",
    "    predicted = model.predict_classes(token_list, verbose = 0)\n",
    "    output_word = \"\"\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted:\n",
    "            output_word = word\n",
    "            break\n",
    "    if output_word == \"newline\":\n",
    "        if already_newline and (not already_bait_baru):\n",
    "            bait += 1\n",
    "            already_bait_baru = True\n",
    "            print(\"Bait {} selesai\".format(bait))\n",
    "        already_newline = True\n",
    "    else:\n",
    "        already_bait_baru = False\n",
    "        already_newline = False\n",
    "        \n",
    "    seed_text += \" \" + output_word\n",
    "    word_count += 1\n",
    "    print(\".\", end=\"\")\n",
    "    \n",
    "print()\n",
    "print(seed_text.replace(\"newline\", \"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"./data/checkpoints/model-checkpoint-with-newline-one-data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x15ee195b978>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights(\"./data/checkpoints/model-checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adalah',\n",
       " 'airmataku',\n",
       " 'akan',\n",
       " 'akankah',\n",
       " 'akhirat',\n",
       " 'aku',\n",
       " 'allah',\n",
       " 'astagfirullah',\n",
       " 'banyak',\n",
       " 'baru',\n",
       " 'berhutang',\n",
       " 'bertambah',\n",
       " 'bertemu',\n",
       " 'besar',\n",
       " 'bisa',\n",
       " 'bulan',\n",
       " 'coba',\n",
       " 'dahiku',\n",
       " 'dalam',\n",
       " 'dan',\n",
       " 'dari',\n",
       " 'daun',\n",
       " 'depan',\n",
       " 'di',\n",
       " 'diberi',\n",
       " 'diriku',\n",
       " 'dunia',\n",
       " 'duniawiku',\n",
       " 'engkau',\n",
       " 'gugur',\n",
       " 'hamba',\n",
       " 'hambamu',\n",
       " 'hari',\n",
       " 'hmm',\n",
       " 'ibadah',\n",
       " 'ibadahku',\n",
       " 'ijikanlah',\n",
       " 'ijin',\n",
       " 'ijinkan',\n",
       " 'ingin',\n",
       " 'ini',\n",
       " 'jatah',\n",
       " 'jauh',\n",
       " 'jika',\n",
       " 'karena',\n",
       " 'kebelakang',\n",
       " 'keinginanku',\n",
       " 'kelemahanku',\n",
       " 'kesempatan',\n",
       " 'khalifahmu',\n",
       " 'khusyuk',\n",
       " 'kuraba',\n",
       " 'kutimbang',\n",
       " 'lebih',\n",
       " 'lembaran',\n",
       " 'lewat',\n",
       " 'masih',\n",
       " 'masihkah',\n",
       " 'melihat',\n",
       " 'membuka',\n",
       " 'mendalam',\n",
       " 'menuju',\n",
       " 'merasakan',\n",
       " 'mulai',\n",
       " 'newline',\n",
       " 'pada',\n",
       " 'pas',\n",
       " 'pasan',\n",
       " 'pasti',\n",
       " 'pelan',\n",
       " 'penyesalanku',\n",
       " 'puncak',\n",
       " 'rasa',\n",
       " 'sama',\n",
       " 'sana',\n",
       " 'sangat',\n",
       " 'satu',\n",
       " 'sebagai',\n",
       " 'sedih',\n",
       " 'sehingga',\n",
       " 'seimbang',\n",
       " 'sempurna',\n",
       " 'semua',\n",
       " 'senyummu',\n",
       " 'sisa',\n",
       " 'sujudku',\n",
       " 'tahun',\n",
       " 'tanda',\n",
       " 'tangga',\n",
       " 'tanggal',\n",
       " 'tapi',\n",
       " 'tengok',\n",
       " 'terjadi',\n",
       " 'ternyata',\n",
       " 'tetes',\n",
       " 'timbangan',\n",
       " 'umurku',\n",
       " 'untuk',\n",
       " 'wajahmu',\n",
       " 'ya',\n",
       " 'yang']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(list(tokenizer.word_index.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hari hari lewat pelan tapi pasti \n",
      " \n",
      " Hari ini aku menuju satu puncak tangga yang baru \n",
      " Karena aku akan membuka lembaran baru \n",
      " Untuk sisa jatah umurku yang baru \n",
      " Daun gugur satu satu \n",
      " Semua terjadi karena ijin Allah \n",
      " Umurku bertambah satu satu \n",
      " Semua terjadi karena ijin Allah \n",
      " Tapi coba aku tengok kebelakang \n",
      " Ternyata aku masih banyak berhutang \n",
      " Ya berhutang pada diriku \n",
      " Karena ibadahku masih pas pasan \n",
      " Kuraba dahiku \n",
      " Astagfirullah sujudku masih jauh dari khusyuk \n",
      " Kutimbang keinginanku \n",
      " Hmm masih lebih besar duniawiku \n",
      " \n",
      " Ya Allah \n",
      " \n",
      " Akankah aku masih bertemu tanggal dan bulan yang sama di tahun depan \n",
      " Akankah aku masih merasakan rasa ini pada tanggal dan bulan yang sama di tahun depan \n",
      " Masihkah aku diberi kesempatan \n",
      " \n",
      " Ya Allah \n",
      " \n",
      " Tetes airmataku adalah tanda kelemahanku \n",
      " Rasa sedih yang mendalam adalah penyesalanku \n",
      " Astagfirullah \n",
      " Jika Engkau ijinkan hamba bertemu tahun depan \n",
      " Ijinkan hambaMU ini mulai hari ini lebih khusyuk dalam ibadah \n",
      " Timbangan dunia dan akhirat hamba seimbang \n",
      " Sehingga hamba bisa sempurna sebagai khalifahMu \n",
      " Hamba sangat ingin melihat wajahMu di sana \n",
      " Hamba sangat ingin melihat senyumMu di sana \n",
      " Ya Allah \n",
      " Ijikanlah \n",
      " \n",
      " \n"
     ]
    }
   ],
   "source": [
    "print(my_sentences[0].replace(\"newline\", \"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
